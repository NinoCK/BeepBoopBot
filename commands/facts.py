import re
from discord.ext import commands
from datetime import datetime
import os
from src.config import conversation_history, user_default_models, BASE_RESPONSE_DIR, USER_MODELS_FILE
from src.utils.file_utils import save_response_to_file
from ollama import chat

# Some models contain headers that may mess up the response text. Could also get the last line of my cmd, but I wanted to use regex instead 
def clean_ollama_response(response_text):
    response_text = re.sub(r'<\|start_header_id\|>.*?<\|end_header_id\|>\n?', '', response_text, flags=re.DOTALL)
    response_text = re.sub(r'<[^>]+>', '', response_text)
    return response_text.strip()

async def send_message(ctx, content):
    if len(content) <= 2000: # Discord message limit
        await ctx.send(content)
    else:
        for i in range(0, len(content), 2000):
            await ctx.send(content[i:i+2000])

@commands.command(name='ask')
async def facts(ctx, *, message):
    user_id = str(ctx.author.id)

    # Activate conversation mode
    conversation_history["active"][user_id] = "facts"

    if user_id not in conversation_history["folders"]:

        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        user_folder = os.path.join(BASE_RESPONSE_DIR, f"ask_{user_id}_{timestamp}")

        os.makedirs(user_folder, exist_ok=True)

        conversation_history["folders"][user_id] = user_folder
    else:
        user_folder = conversation_history["folders"][user_id]

    if user_id not in conversation_history["facts"]:
        conversation_history["facts"][user_id] = [{'role': 'system', 'content': 'You are a knowledgeable assistant. Provide accurate and interesting facts.'}]

    pdf_context = ""

    for msg in conversation_history["facts"][user_id]:

        if msg["role"] == "system" and "Additional knowledge from uploaded PDF" in msg["content"]:
            pdf_context += msg["content"] + "\n"

    conversation_history["facts"][user_id].append({'role': 'user', 'content': message})

    full_prompt = [{'role': 'system', 'content': "You are a helpful AI assistant. Maintain context across responses."},]
    full_prompt += conversation_history["facts"][user_id] 

    model = user_default_models.get(user_id, 'llama3.2:latest') # Default model of preference, can be changed by the user

    try:
        response = chat(model=model, messages=full_prompt)
        bot_response = response['message']['content']

        bot_response = clean_ollama_response(bot_response)

        conversation_history["facts"][user_id].append({'role': 'assistant', 'content': bot_response})

        await send_message(ctx, f"**Response generated by model:** `{model}`\n\n{bot_response}")
        save_response_to_file('ask', user_id, message, bot_response, user_folder)
    
    except Exception as e:
        await ctx.send(f"An error occurred: {e}")
